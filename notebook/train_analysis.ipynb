{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c65ca5",
   "metadata": {},
   "source": [
    "# Eğitim ve Veri Analizi Not Defteri\n",
    "Bu not defteri `output/` ve `models/` altındaki çıktı dosyalarını inceleyerek: veri keşfi (EDA), eğitim loglarının görselleştirilmesi ve örnek çıkarım adımlarını içerir.\n",
    "\n",
    "Not: Hücreler, dosyalar yoksa hatasız çalışacak şekilde kontroller ve `try/except` blokları içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199f4534",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Cell: imports and output paths\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Default output dir used by the training script (adjust if you used a different OUTPUT_DIR)\n",
    "OUT_DIR = Path('outputs') / 'multitask-lora-fast'\n",
    "metrics_csv = OUT_DIR / 'metrics.csv'\n",
    "eval_jsonl = OUT_DIR / 'eval_predictions.jsonl'\n",
    "test_metrics_txt = OUT_DIR / 'test_metrics.txt'\n",
    "\n",
    "print('OUT_DIR =', OUT_DIR)\n",
    "print('metrics.csv exists?', metrics_csv.exists())\n",
    "print('eval_predictions.jsonl exists?', eval_jsonl.exists())\n",
    "print('test_metrics.txt exists?', test_metrics_txt.exists())\n",
    "\n",
    "# List available files in output dir (helpful quick check)\n",
    "if OUT_DIR.exists():\n",
    "    files = sorted(OUT_DIR.glob('*'))\n",
    "    print(f\"Found {len(files)} items in {OUT_DIR}:\")\n",
    "    for p in files:\n",
    "        try:\n",
    "            size = p.stat().st_size\n",
    "        except Exception:\n",
    "            size = 0\n",
    "        print(f\" - {p.name}\\t{size:,} bytes\")\n",
    "else:\n",
    "    print('Output directory does not exist yet.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140ae7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: Load metrics.csv if available\u001b[39;00m\n\u001b[32m      2\u001b[39m metrics_df = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmetrics_csv\u001b[49m.exists():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         metrics_df = pd.read_csv(metrics_csv)\n",
      "\u001b[31mNameError\u001b[39m: name 'metrics_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: Load metrics.csv if available\n",
    "metrics_df = None\n",
    "try:\n",
    "    if metrics_csv.exists():\n",
    "        metrics_df = pd.read_csv(metrics_csv)\n",
    "        metrics_df.columns = [c.strip() for c in metrics_df.columns]\n",
    "        display(metrics_df.head())\n",
    "        display(metrics_df.describe(include='all'))\n",
    "        for col in ['train_loss','eval_loss','train_em','eval_em','train_token_f1','eval_token_f1','train_rougeL_f1','eval_rougeL_f1']:\n",
    "            if col in metrics_df.columns:\n",
    "                metrics_df[col] = pd.to_numeric(metrics_df[col], errors='coerce')\n",
    "    else:\n",
    "        print('metrics.csv not found in', OUT_DIR)\n",
    "except Exception as e:\n",
    "    print('Failed to read metrics.csv:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2826fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Helper function: generate and save metric plots (loss + optional metric plots)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Call: plot_train_eval_metrics(metrics_csv, OUT_DIR, window=5)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_train_eval_metrics\u001b[39m(metrics_csv=\u001b[43mmetrics_csv\u001b[49m, out_dir=OUT_DIR, window=\u001b[32m5\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'metrics_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper function: generate and save metric plots (loss + optional metric plots)\n",
    "# Call: plot_train_eval_metrics(metrics_csv, OUT_DIR, window=5)\n",
    "def plot_train_eval_metrics(metrics_csv=metrics_csv, out_dir=OUT_DIR, window=5):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    PLOT_DIR = out_dir / 'analysis_plots'\n",
    "    PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        if metrics_csv.exists():\n",
    "            df = pd.read_csv(metrics_csv)\n",
    "        else:\n",
    "            print(f'Metrics CSV not found at {metrics_csv}')\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read metrics CSV at {metrics_csv}: {e}')\n",
    "        return []\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    steps = df['step'] if 'step' in df.columns else np.arange(len(df))\n",
    "    saved = []\n",
    "    # Loss plot\n",
    "    if 'train_loss' in df.columns and 'eval_loss' in df.columns:\n",
    "        train_smooth = df['train_loss'].rolling(window=window, min_periods=1).mean()\n",
    "        eval_smooth = df['eval_loss'].rolling(window=window, min_periods=1).mean()\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(steps, df['train_loss'], color='C0', alpha=0.25, label='train_loss (raw)')\n",
    "        plt.plot(steps, df['eval_loss'], color='C1', alpha=0.25, label='eval_loss (raw)')\n",
    "        plt.plot(steps, train_smooth, color='C0', label=f'train_loss (smoothed w={window})')\n",
    "        plt.plot(steps, eval_smooth, color='C1', label=f'eval_loss (smoothed w={window})')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train vs Eval Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        outp = PLOT_DIR / 'train_eval_loss.png'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outp)\n",
    "        plt.close()\n",
    "        saved.append(str(outp))\n",
    "    metric_pairs = [\n",
    "        ('train_em','eval_em','Exact Match (EM)'),\n",
    "        ('train_token_f1','eval_token_f1','Token F1'),\n",
    "        ('train_rougeL_f1','eval_rougeL_f1','RougeL F1'),\n",
    "    ]\n",
    "    for tcol, ecol, title in metric_pairs:\n",
    "        if tcol in df.columns and ecol in df.columns:\n",
    "            plt.figure(figsize=(10,3))\n",
    "            plt.plot(steps, df[tcol], alpha=0.4, label=f'train_{title}')\n",
    "            plt.plot(steps, df[ecol], alpha=0.6, label=f'eval_{title}')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel(title)\n",
    "            plt.title(title + ' over steps')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            fname = title.lower().replace(' ','_') + '.png'\n",
    "            ppath = PLOT_DIR / fname\n",
    "            plt.savefig(ppath)\n",
    "            plt.close()\n",
    "            saved.append(str(ppath))\n",
    "    print('Saved plots:', saved)\n",
    "    return saved\n",
    "# Optionally regenerate plots immediately if you want uncomment the next line\n",
    "# plot_train_eval_metrics(metrics_csv, OUT_DIR, window=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b21508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics dataframe to plot.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Plot training and validation curves if metrics available\n",
    "if metrics_df is not None and not metrics_df.empty:\n",
    "    df = metrics_df.copy()\n",
    "    PLOT_DIR = OUT_DIR / 'analysis_plots'\n",
    "    PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Steps vector\n",
    "    steps = df['step'] if 'step' in df.columns else np.arange(len(df))\n",
    "\n",
    "    # Loss plot (raw + smoothed)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if 'train_loss' in df.columns and 'eval_loss' in df.columns:\n",
    "        window = 5\n",
    "        train_smooth = df['train_loss'].rolling(window=window, min_periods=1).mean()\n",
    "        eval_smooth = df['eval_loss'].rolling(window=window, min_periods=1).mean()\n",
    "        plt.plot(steps, df['train_loss'], alpha=0.25, label='train_loss (raw)')\n",
    "        plt.plot(steps, df['eval_loss'], alpha=0.25, label='eval_loss (raw)')\n",
    "        plt.plot(steps, train_smooth, label=f'train_loss (smoothed w={window})')\n",
    "        plt.plot(steps, eval_smooth, label=f'eval_loss (smoothed w={window})')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train vs Eval Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        loss_path = PLOT_DIR / 'train_eval_loss.png'\n",
    "        plt.savefig(loss_path)\n",
    "        plt.show()\n",
    "        print('Saved loss plot to', loss_path)\n",
    "    else:\n",
    "        print('train_loss or eval_loss not present in metrics_df; columns:', df.columns.tolist())\n",
    "\n",
    "    # Metric plots if present\n",
    "    metric_pairs = [\n",
    "        ('train_em','eval_em','Exact Match (EM)'),\n",
    "        ('train_token_f1','eval_token_f1','Token F1'),\n",
    "        ('train_rougeL_f1','eval_rougeL_f1','RougeL F1'),\n",
    "    ]\n",
    "    for tcol, ecol, title in metric_pairs:\n",
    "        if tcol in df.columns and ecol in df.columns:\n",
    "            plt.figure(figsize=(10,3))\n",
    "            plt.plot(steps, df[tcol], alpha=0.4, label=f'train_{title}')\n",
    "            plt.plot(steps, df[ecol], alpha=0.6, label=f'eval_{title}')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel(title)\n",
    "            plt.title(title + ' over steps')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            fname = title.lower().replace(' ','_') + '.png'\n",
    "            ppath = PLOT_DIR / fname\n",
    "            plt.savefig(ppath)\n",
    "            plt.show()\n",
    "            print('Saved', ppath)\n",
    "\n",
    "    # Final values summary\n",
    "    last = df.iloc[-1]\n",
    "    print('\\nFinal step summary:')\n",
    "    vals = {}\n",
    "    for col in ['train_loss','eval_loss','train_em','eval_em','train_token_f1','eval_token_f1']:\n",
    "        if col in last.index:\n",
    "            vals[col] = last[col]\n",
    "    print(vals)\n",
    "else:\n",
    "    print('No metrics dataframe to plot.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca9c679",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_jsonl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: Load eval_predictions.jsonl and show sample errors (low EM or low token_f1)\u001b[39;00m\n\u001b[32m      2\u001b[39m preds = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43meval_jsonl\u001b[49m.exists():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(eval_jsonl, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'eval_jsonl' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: Load eval_predictions.jsonl and show sample errors (low EM or low token_f1)\n",
    "preds = []\n",
    "if eval_jsonl.exists():\n",
    "    with open(eval_jsonl, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                preds.append(obj)\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(f'Loaded {len(preds)} eval predictions')\n",
    "    if preds:\n",
    "        dfp = pd.DataFrame(preds)\n",
    "        # normalize\n",
    "        if 'em' in dfp.columns:\n",
    "            dfp['em'] = pd.to_numeric(dfp['em'], errors='coerce')\n",
    "        if 'token_f1' in dfp.columns:\n",
    "            dfp['token_f1'] = pd.to_numeric(dfp['token_f1'], errors='coerce')\n",
    "\n",
    "        # token overlap measure (simple)\n",
    "        def token_overlap(p, t):\n",
    "            ps = set(str(p).lower().split())\n",
    "            ts = set(str(t).lower().split())\n",
    "            if not ts: return 0.0\n",
    "            return len(ps & ts) / len(ts)\n",
    "\n",
    "        dfp['tok_overlap'] = dfp.apply(lambda r: token_overlap(r.get('prediction',''), r.get('target','')), axis=1)\n",
    "        display(dfp.head())\n",
    "\n",
    "        # Worst by token_f1\n",
    "        if 'token_f1' in dfp.columns:\n",
    "            worst = dfp.nsmallest(10, 'token_f1')\n",
    "            print('\\nWorst 10 by token_f1:')\n",
    "            display(worst[['source','target','prediction','em','token_f1','tok_overlap']])\n",
    "\n",
    "        # Random incorrect examples\n",
    "        if 'em' in dfp.columns:\n",
    "            wrong = dfp[dfp['em'] == 0]\n",
    "            if len(wrong) > 0:\n",
    "                sample_wrong = wrong.sample(min(10, len(wrong)), random_state=42)\n",
    "                print('\\nRandom sample of incorrect predictions (EM==0):')\n",
    "                display(sample_wrong[['source','target','prediction','em','token_f1','tok_overlap']])\n",
    "\n",
    "        # Distribution summaries\n",
    "        print('\\nToken F1 distribution:')\n",
    "        if 'token_f1' in dfp.columns:\n",
    "            display(dfp['token_f1'].describe())\n",
    "        print('\\nToken overlap distribution:')\n",
    "        display(dfp['tok_overlap'].describe())\n",
    "else:\n",
    "    print('eval_predictions.jsonl not found in', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21da517d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_metrics_txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: Read test_metrics.txt (if produced) and show contents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtest_metrics_txt\u001b[49m.exists():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mContents of\u001b[39m\u001b[33m'\u001b[39m, test_metrics_txt)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m40\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_metrics_txt' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: Read test_metrics.txt (if produced) and show contents\n",
    "if test_metrics_txt.exists():\n",
    "    print('Contents of', test_metrics_txt)\n",
    "    print('-'*40)\n",
    "    print(test_metrics_txt.read_text(encoding='utf-8'))\n",
    "else:\n",
    "    print('No test_metrics.txt found in', OUT_DIR)\n",
    "\n",
    "# Recommendations based on observed training outputs\n",
    "print('\\nQuick recommendations based on observed artifacts:')\n",
    "if metrics_df is not None and not metrics_df.empty:\n",
    "    last = metrics_df.iloc[-1]\n",
    "    tr = last.get('train_loss', None)\n",
    "    ev = last.get('eval_loss', None)\n",
    "    if tr is not None and ev is not None and ev > tr * 1.2:\n",
    "        print('- Validation loss significantly higher than training loss → possible overfitting. Try lower LR or stronger regularization.')\n",
    "    if 'eval_token_f1' in last.index and last['eval_token_f1'] < 0.3:\n",
    "        print('- Low eval token_f1 (<0.3) — consider increasing MAX_SOURCE_LEN or improving data quality.')\n",
    "print('- To regenerate plots run the plotting cells; plots are saved under:', OUT_DIR / 'analysis_plots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86ece0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: Dedicated Train vs Eval Loss plot (raw + smoothed)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ensure metrics_df is loaded or try to load\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Cell: Dedicated Train vs Eval Loss plot (raw + smoothed)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure metrics_df is loaded or try to load\n",
    "if 'metrics_df' not in globals() or metrics_df is None:\n",
    "    if metrics_csv.exists():\n",
    "        try:\n",
    "            metrics_df = pd.read_csv(metrics_csv)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load metrics.csv: {e}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"metrics.csv not found at {metrics_csv}\")\n",
    "\n",
    "df = metrics_df.copy()\n",
    "if 'train_loss' not in df.columns or 'eval_loss' not in df.columns:\n",
    "    print('Columns available in metrics.csv:', df.columns.tolist())\n",
    "    raise RuntimeError('train_loss and/or eval_loss not present in metrics.csv')\n",
    "\n",
    "steps = df['step'] if 'step' in df.columns else np.arange(len(df))\n",
    "window = 5  # smoothing window (adjust if needed)\n",
    "train_smooth = df['train_loss'].rolling(window=window, min_periods=1).mean()\n",
    "eval_smooth = df['eval_loss'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(steps, df['train_loss'], color='C0', alpha=0.25, label='train_loss (raw)')\n",
    "plt.plot(steps, df['eval_loss'], color='C1', alpha=0.25, label='eval_loss (raw)')\n",
    "plt.plot(steps, train_smooth, color='C0', label=f'train_loss (smoothed w={window})')\n",
    "plt.plot(steps, eval_smooth, color='C1', label=f'eval_loss (smoothed w={window})')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Eval Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "PLOT_DIR = OUT_DIR / 'analysis_plots'\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "outp = PLOT_DIR / 'train_eval_loss.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(outp)\n",
    "plt.show()\n",
    "print('Saved plot to', outp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ee2f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: Display saved plots from analysis_plots (so you can quickly view them inline)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m PLOT_DIR = \u001b[43mOUT_DIR\u001b[49m / \u001b[33m'\u001b[39m\u001b[33manalysis_plots\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PLOT_DIR.exists():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNo plots found at\u001b[39m\u001b[33m'\u001b[39m, PLOT_DIR)\n",
      "\u001b[31mNameError\u001b[39m: name 'OUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: Display saved plots from analysis_plots (so you can quickly view them inline)\n",
    "from IPython.display import Image, display\n",
    "PLOT_DIR = OUT_DIR / 'analysis_plots'\n",
    "\n",
    "if not PLOT_DIR.exists():\n",
    "    print('No plots found at', PLOT_DIR)\n",
    "else:\n",
    "    imgs = sorted(PLOT_DIR.glob('*.png'))\n",
    "    if not imgs:\n",
    "        print('No PNG files found in', PLOT_DIR)\n",
    "    for p in imgs:\n",
    "        print('\\n==', p.name, '==')\n",
    "        try:\n",
    "            display(Image(str(p)))\n",
    "        except Exception as e:\n",
    "            print('Failed to display', p, e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
